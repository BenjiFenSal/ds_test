<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>McNemar's Test - Blog</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.8;
            margin: 40px;
            background: #f4f4f4;
            color: #333;
        }

        .container {
            max-width: 900px;
            margin: auto;
            background: #fff;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 0 15px rgba(0, 0, 0, 0.1);
        }

        h1, h2, h3 {
            color: #222;
            border-bottom: 2px solid #ddd;
            padding-bottom: 5px;
        }

        pre {
            background: #1e1e1e;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 1rem;
        }

        code {
            font-family: Consolas, "Courier New", monospace;
        }

        ul {
            margin-left: 20px;
            padding-left: 20px;
        }

        p {
            font-size: 1.1rem;
        }

        .collapsible-header {
            cursor: pointer;
            padding: 10px;
            background-color: #f0f0f0;
            border: 1px solid #ddd;
            margin-bottom: 0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .collapsible-content {
            padding: 10px;
            border: 1px solid #ddd;
            border-top: none;
            display: none;
        }

        .collapsible-content.show {
            display: block;
        }

        .toggle-icon {
            font-weight: bold;
        }
    </style>

    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
    <div class="container">
        <h1>McNemar's Test Tutorial</h1>

        <p><strong>Module:</strong> Practical Data Science in Chemistry with Python</p>

        <div class="collapsible">
            <h2 class="collapsible-header">Introduction <span class="toggle-icon">+</span></h2>
            <div class="collapsible-content">
                <p>McNemar's Test is a statistical method used to analyse paired nominal (categorical) data, often applied in "before-and-after" or "matched pairs" scenarios. For example, consider evaluating whether a new in vitro diagnostic (IVD) test improves selectivity compared to an existing gold-standard test, using the same set of samples.</p>
                <p>This test focuses on changes in responses rather than absolute values, making it ideal for cases where subjects (e.g., samples, experiments) act as their own controls.</p>
            </div>
        </div>

        <div class="collapsible">
            <h2 class="collapsible-header">Key Idea (Simple Terms) <span class="toggle-icon">+</span></h2>
            <div class="collapsible-content">
                <p>McNemar’s Test identifies differences in yes/no outcomes when testing the same group under different conditions (e.g., old vs. new method).</p>
            </div>
        </div>

        <div class="collapsible">
            <h2 class="collapsible-header">Technical Definition <span class="toggle-icon">+</span></h2>
            <div class="collapsible-content">
                <p>McNemar's Test is a non-parametric test for paired dichotomous data, assessing marginal homogeneity in a 2x2 contingency table.</p>
            </div>
        </div>

        <div class="collapsible">
            <h2 class="collapsible-header">When to Use McNemar’s Test <span class="toggle-icon">+</span></h2>
            <div class="collapsible-content">
                <p>You should use McNemar’s Test when:</p>
                <ul>
                    <li>✅ You have paired data (e.g., the same participants or samples measured twice).</li>
                    <li>✅ The variable you are measuring is categorical with two possible values (e.g., Yes/No, Pass/Fail, Success/Failure).</li>
                    <li>✅ You want to determine whether the proportions changed significantly after an intervention.</li>
                </ul>
            </div>
        </div>

        <div class="collapsible">
            <h2 class="collapsible-header">CASE STUDY: Evaluating Selectivity in In Vitro Diagnostics (IVD) Using McNemar’s Test <span class="toggle-icon">+</span></h2>
            <div class="collapsible-content">
                <h3>Real-World Example: Comparing a New Diagnostic Test to a Gold-Standard Method</h3>
                <p>Imagine a laboratory developing a new diagnostic test (Test A) for detecting a viral infection. To assess its performance, the lab compares it against an established gold-standard test (Test B). The primary focus is selectivity—the ability to correctly identify negative samples and minimize false positives.</p>
                <p>The study involves 100 patient samples, each tested with both methods. The results are recorded as positive (1) or negative (0) for each test. The goal is to determine whether Test A and Test B classify negative samples differently and, if so, whether the difference is statistically significant.</p>

                <h3>Understanding Selectivity in This Context</h3>

                <h4>What is Selectivity?</h4>
                <ul>
                    <li><strong>Technical Definition:</strong> Selectivity in diagnostics refers to a test’s ability to correctly classify negative samples, ensuring it does not falsely identify them as positive. It is closely related to specificity, which quantifies the proportion of true negatives correctly identified.</li>
                    <li><strong>Intuitive Explanation:</strong> A highly selective test avoids false alarms—it correctly distinguishes between truly negative and falsely positive results. </li>
                </ul>

                <h4>Why McNemar’s Test?</h4>
                <ul>
                    <li>When comparing two diagnostic methods on the same set of samples, McNemar’s Test is ideal because it focuses on paired categorical data (positive/negative classifications).</li>
                    <li>It assesses whether the disagreement in classification between the tests is balanced or biased—if one test systematically classifies more negative samples as positive, it suggests a difference in selectivity.</li>
                </ul>
            </div>
        </div>

        <div class="collapsible">
            <h2 class="collapsible-header">Research Question, Hypotheses, and Predictions <span class="toggle-icon">+</span></h2>
            <div class="collapsible-content">
                <p>Before conducting McNemar’s Test, it's essential to clearly define the research question, establish null and alternative hypotheses, and outline expected outcomes. This ensures methodological rigor and interpretable results.</p>

                <h2>Research Question</h2>

                <p>"Does the new diagnostic test (Test A) differ significantly from the gold-standard test (Test B) in correctly classifying negative samples (selectivity)?"</p>

                <ul>
                    <li><strong>Primary focus:</strong> Whether Test A misclassifies negative samples more or less frequently than Test B.</li>
                    <li><strong>Importance:</strong> In clinical diagnostics, false positives can lead to unnecessary treatments, while false negatives may delay critical care.</li>
                </ul>

                <h2>Null and Alternative Hypotheses</h2>

                <h3>Null Hypothesis (H₀)</h3>

                <p>"There is no significant difference in selectivity between Test A and Test B."</p>

                <p>Mathematically, this means:</p>

		<p>
    		$$ P_{\text{A → Positive | B → Negative}} = P_{\text{A → Negative | B → Positive}} $$
		</p>

                <p>Or equivalently, in terms of discordant pairs:</p>

                <p>
                    <span class="math-block">b \= c</span>
                </p>

                <p>where:</p>

                <ul>
                    <li><span class="math-inline">b</span>: Cases where Test A classifies negative, but Test B classifies positive.</li>
                    <li><span class="math-inline">c</span>: Cases where Test A classifies positive, but Test B classifies negative.</li>
                </ul>

                <h3>Alternative Hypothesis (H₁)</h3>

                <p>"There is a significant difference in selectivity between Test A and Test B."</p>

                <p>This suggests:</p>

                <p>
                    $$P_{\text{A → Positive | B → Negative}} \neq P_{\text{A → Negative | B → Positive}}$$
                </p>

                <p>Or in discordant pairs:</p>

                <p>
                    $$b \neq c$$
                </p>

                <p>A significant result would indicate that one test systematically classifies more negative samples as positive than the other.</p>

                <h2>Predictions Based on Different Outcomes</h2>

                <table>
                    <thead>
                        <tr>
                            <th>Outcome</th>
                            <th>Interpretation</th>
                            <th>Decision</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>b ≈ c, p ≥ 0.05</td>
                            <td>No significant difference in selectivity.</td>
                            <td>Fail to reject H₀. Test A performs similarly to Test B.</td>
                        </tr>
                        <tr>
                            <td>b > c, p < 0.05</td>
                            <td>Test A is more selective than Test B (fewer false positives).</td>
                            <td>Reject H₀. Test A has significantly better selectivity.</td>
                        </tr>
                        <tr>
                            <td>b < c, p < 0.05</td>
                            <td>Test A is less selective than Test B (more false positives).</td>
                            <td>Reject H₀. Test A has significantly worse selectivity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="collapsible">
            <h2 class="collapsible-header">Step 1: Load and Organize Data <span class="toggle-icon">+</span></h2>
            <div class="collapsible-content">
                <p>First, we load the dataset, which contains test results from two diagnostic tests (Test A and Test B). These results are recorded as binary outcomes:</p>

                <ul>
                    <li>1 = Positive result</li>
                    <li>0 = Negative result</li>
                </ul>

                <p>The dataset should be stored as a CSV file (McNemars_IVD_synthetic_data.csv), where each row represents a patient, and each column represents a test result.</p>

                <pre><code>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv("McNemars_IVD_synthetic_data.csv")
</code></pre>

            </div>
        </div>


<div class="collapsible">
    <h2 class="collapsible-header">Step 2: Construct the 2×2 Contingency Table<span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <p>A contingency table is used to compare the results of Test A and Test B. The <code>pd.crosstab()</code> function is used to generate this table, summarizing how often each test agrees or disagrees.</p>

 <pre><code># Create the 2x2 contingency table
contingency_table = pd.crosstab(df["Test A"], df["Test B"])

# Rename index and column labels for clarity
contingency_table.index = ["Test A Negative (0)", "Test A Positive (1)"]
contingency_table.columns = ["Test B Negative (0)", "Test B Positive (1)"]
</code></pre>    
       
 <p>This will generate a table in the following format:</p>
        <table>
            <thead>
                <tr>
                    <th>Test A \ Test B</th>
                    <th>Negative (0)</th>
                    <th>Positive (1)</th>
                    <th>Total</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <th>Negative (0)</th>
                    <td>85</td>
                    <td>5</td>
                    <td>90</td>
                </tr>
                <tr>
                    <th>Positive (1)</th>
                    <td>3</td>
                    <td>7</td>
                    <td>10</td>
                </tr>
                <tr>
                    <th>Total</th>
                    <td>88</td>
                    <td>12</td>
                    <td>100</td>
                </tr>
            </tbody>
        </table>
    </div>
</div>

<div class="collapsible">
    <h2 class="collapsible-header">Step 3: Compute Proportions<span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <p>To better understand the distribution of test results, we calculate proportions by dividing each count by the total number of observations.</p>
        <p>This outputs a normalized version of the table where each value represents the percentage of total cases in that category.</p>

        <pre><code># Calculate the total number of observations
total_counts = contingency_table.sum().sum()

# Calculate the proportions for each category
proportions = contingency_table / total_counts
</code></pre>
    </div>
</div>

<div class="collapsible">
    <h2 class="collapsible-header">Step 4: Visualize the Contingency Table<span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <p>To make the results easier to interpret, we generate a heatmap using Seaborn (<code>sns.heatmap</code>). This provides a visual representation of how frequently each test result combination occurs.</p>
        <p>This heatmap helps identify where discrepancies between the two tests occur.</p>

        <pre><code># Heatmap of the Contingency Table
plt.figure(figsize=(5,4))
sns.heatmap(contingency_table, annot=True, fmt='d', cmap="Blues", linewidths=0.5)
plt.title("Contingency Table Heatmap")
plt.show()
</code></pre>
    </div>
</div>

<div class="collapsible">
    <h2 class="collapsible-header">Next Steps: McNemar’s Test<span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <p>To determine if there is a statistically significant difference in test selectivity, McNemar’s test should be applied. This requires calculating:</p>
        <ul>
            <li>b (false negatives) = 5</li>
            <li>c (false positives) = 3</li>
        </ul>
        <p>Then, using:</p>
        <p>
            $$\chi^2 = \frac{(b - c)^2}{b + c}$$
        </p>
        <p>which can be implemented in Python using <code>scipy.stats</code>.</p>
        <p>If χ² < 3.84 (at α = 0.05), we fail to reject the null hypothesis, meaning there is no significant difference between the tests.</p>

        <pre><code>from scipy.stats import chi2_contingency

b, c = contingency_table.iloc[0, 1], contingency_table.iloc[1, 0]  # False negatives and false positives
chi2_stat = (abs(b - c) - 1) ** 2 / (b + c)  # Correction for small sample sizes
p_value = 1 - chi2_contingency(contingency_table, correction=False)[1]  # Chi-square test

print(f"Chi-square statistic: {chi2_stat:.3f}")
print(f"P-value: {p_value:.3f}")
</code></pre>
    </div>
</div>

<div class="collapsible">
    <h2 class="collapsible-header">Summary of Practical Implementation<span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <p>Here's a summary of the practical steps involved:</p>
        <ol>
            <li>Load the data and ensure it contains two binary test result columns.</li>
            <li>Create a contingency table to compare Test A vs. Test B.</li>
            <li>Compute proportions for better interpretation.</li>
            <li>Visualize the table using a heatmap.</li>
            <li>Perform McNemar’s test to determine if Test A and Test B differ significantly.</li>
        </ol>
    </div>
</div>

<div class="collapsible">
    <h2 class="collapsible-header">Key Takeaways <span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <ul>
            <li>✔ McNemar’s Test is ideal for comparing the selectivity of two diagnostic tests using the same set of samples.</li>
            <li>✔ It focuses on discordant pairs rather than overall classification accuracy.</li>
            <li>✔ Use the exact test when b + c < 25 for better accuracy.</li>
            <li>✔ A small p-value (< 0.05) suggests a real difference in test performance.</li>
            <li>✔ McNemar’s Test does not measure effect size, only whether a significant difference exists.</li>
        </ul>
    </div>
</div>

<div class="collapsible">
    <h2 class="collapsible-header">Limitations of McNemar’s Test <span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <ul>
            <li>Only applies to paired binary data.
                <ul>
                    <li>If data has more than two categories, use an alternative test.</li>
                    <li>If data is not paired, use a Chi-Square test instead.</li>
                </ul>
            </li>
            <li>Sensitive to small sample sizes—for b + c < 25, use the exact binomial test.</li>
            <li>Does not measure effect size—it detects differences but does not quantify them.</li>
            <li>Cannot determine causation—only association.</li>
        </ul>
    </div>
</div>

<div class="collapsible">
    <h2 class="collapsible-header">Glossary of Key Terms <span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <ol>
            <li><strong>McNemar’s Test</strong>
                <ul>
                    <li>Formal Definition: A statistical test used to compare paired binary data to assess whether there is a significant difference in classification proportions between two conditions. It is a non-parametric test that examines marginal homogeneity in a 2×2 contingency table.</li>
                    <li>Intuitive Explanation: Think of it as a way to check if two tests (or methods) classify the same samples differently, focusing only on cases where they disagree.</li>
                </ul>
            </li>
            <li><strong>Paired Data</strong>
                <ul>
                    <li>Formal Definition: Data where each observation in one condition is directly linked to an observation in another condition, meaning the two measurements come from the same subject or sample.</li>
                    <li>Intuitive Explanation: Each test result comes in a "before-and-after" pair—like testing the same patient with two different diagnostic methods.</li>
                </ul>
            </li>
            <li><strong>Binary Data</strong>
                <ul>
                    <li>Formal Definition: Data with only two possible outcomes, often represented as 0 and 1 (e.g., Positive/Negative, Success/Failure).</li>
                    <li>Intuitive Explanation: A simple yes-or-no decision, like whether a test result is positive or negative.</li>
                </ul>
            </li>
            <li><strong>Contingency Table</strong>
                <ul>
                    <li>Formal Definition: A matrix used to display the frequency distribution of categorical data, summarizing how two variables interact.</li>
                    <li>Intuitive Explanation: A grid that organizes test results, showing where the two tests agree and where they disagree.</li>
                </ul>
            </li>
            <li><strong>Discordant Pairs</strong>
                <ul>
                    <li>Formal Definition: Observations where two paired tests produce different results (one test is positive, the other is negative).</li>
                    <li>Intuitive Explanation: Cases where one test says "yes" and the other says "no"—these are the only results that matter in McNemar’s Test.</li>
                </ul>
            </li>
            <li><strong>Null Hypothesis (H₀)</strong>
                <ul>
                    <li>Formal Definition: The assumption that there is no significant difference between the two paired conditions (i.e., both tests perform similarly).</li>
                    <li>Intuitive Explanation: The starting assumption that any difference between the tests is due to random chance.</li>
                </ul>
            </li>
            <li><strong>Alternative Hypothesis (H₁)</strong>
                <ul>
                    <li>Formal Definition: The hypothesis stating that a significant difference exists between the two paired conditions.</li>
                    <li>Intuitive Explanation: The idea that one test really does perform differently (better or worse) than the other.</li>
                </ul>
            </li>
            <li><strong>Chi-Squared Statistic (χ²)</strong>
                <ul>
                    <li>Formal Definition: A statistical measure that quantifies the difference between observed and expected frequencies in categorical data. For McNemar’s Test, it measures the imbalance in discordant pairs.</li>
                    <li>Intuitive Explanation: A way to see how much disagreement between the tests is due to real differences rather than random chance.</li>
                </ul>
            </li>
            <li><strong>p-value</strong>
                <ul>
                    <li>Formal Definition: The probability of obtaining results as extreme as (or more extreme than) the observed ones, assuming the null hypothesis is true.</li>
                    <li>Intuitive Explanation: A number that tells us whether the difference between the tests is likely just random (high p-value) or actually meaningful (low p-value).</li>
                </ul>
            </li>
            <li><strong>Exact Binomial Test</strong>
                <ul>
                    <li>Formal Definition: A test used when sample sizes are small (b + c < 25), treating the discordant cases as outcomes of a binomial process with a 50% expected probability.</li>
                    <li>Intuitive Explanation: A more precise version of McNemar’s Test for small datasets—like flipping a coin multiple times to see if one side comes up significantly more often than expected.</li>
                </ul>
            </li>
            <li><strong>Selectivity</strong>
                <ul>
                    <li>Formal Definition: The ability of a diagnostic test to correctly classify negative cases, minimizing false positives.</li>
                    <li>Intuitive Explanation: How well a test avoids mistakenly identifying something as positive when it’s actually negative.</li>
                </ul>
            </li>
            <li><strong>Effect Size</strong>
                <ul>
                    <li>Formal Definition: A quantitative measure of the strength or magnitude of a difference between groups, often used to complement significance tests.</li>
                    <li>Intuitive Explanation: Even if two tests are different, how big is the difference? McNemar’s Test tells us if there’s a difference, but effect size tells us how important that difference is.</li>
                </ul>
            </li>
        </ol>
    </div>
</div>


<div class="collapsible">
    <h2 class="collapsible-header">Try it out! Example Code in Full<span class="toggle-icon">+</span></h2>
    <div class="collapsible-content">
        <pre><code>import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv("McNemars_IVD_synthetic_data.csv")

# Create the 2x2 contingency table
contingency_table = pd.crosstab(df["Test A"], df["Test B"])
contingency_table.index = ["Test A Negative (0)", "Test A Positive (1)"]
contingency_table.columns = ["Test B Negative (0)", "Test B Positive (1)"]

# Calculate the total number of observations
total_counts = contingency_table.sum().sum()

# Calculate the proportions for each category
proportions = contingency_table / total_counts

# Display the contingency table and proportions
print("Contingency Table:\n", contingency_table)
print("\nProportions Table:\n", proportions)

# Heatmap of the Contingency Table
plt.figure(figsize=(5,4))
sns.heatmap(contingency_table, annot=True, fmt='d', cmap="Blues", linewidths=0.5)
plt.title("Contingency Table Heatmap")
plt.show()
</code></pre>
    </div>
</div>









    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const collapsibleHeaders = document.querySelectorAll('.collapsible-header');

            collapsibleHeaders.forEach(header => {
                header.addEventListener('click', function() {
                    const content = this.nextElementSibling;
                    const icon = this.querySelector('.toggle-icon');

                    content.classList.toggle('show');

                    if (content.classList.contains('show')) {
                        icon.textContent = '-';
                    } else {
                        icon.textContent = '+';
                    }
                });
            });
        });
    </script>
</body>
</html>
